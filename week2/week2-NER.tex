\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{week2-NER}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \hypertarget{recognize-named-entities-on-twitter-with-lstms}{%
\section{Recognize named entities on Twitter with
LSTMs}\label{recognize-named-entities-on-twitter-with-lstms}}

In this assignment, you will use a recurrent neural network to solve
Named Entity Recognition (NER) problem. NER is a common task in natural
language processing systems. It serves for extraction such entities from
the text as persons, organizations, locations, etc. In this task you
will experiment to recognize named entities from Twitter.

For example, we want to extract persons' and organizations' names from
the text. Than for the input text:

\begin{verbatim}
Ian Goodfellow works for Google Brain
\end{verbatim}

a NER model needs to provide the following sequence of tags:

\begin{verbatim}
B-PER I-PER    O     O   B-ORG  I-ORG
\end{verbatim}

Where \emph{B-} and \emph{I-} prefixes stand for the beginning and
inside of the entity, while \emph{O} stands for out of tag or no tag.
Markup with the prefix scheme is called \emph{BIO markup}. This markup
is introduced for distinguishing of consequent entities with similar
types.

A solution of the task will be based on neural networks, particularly,
on Bi-Directional Long Short-Term Memory Networks (Bi-LSTMs).

\hypertarget{libraries}{%
\subsubsection{Libraries}\label{libraries}}

For this task you will need the following libraries: -
\href{https://www.tensorflow.org}{Tensorflow} --- an open-source
software library for Machine Intelligence.

In this assignment, we use Tensorflow 1.15.0. You can install it with
pip:

\begin{verbatim}
!pip install tensorflow==1.15.0
 
\end{verbatim}

\begin{itemize}
\tightlist
\item
  \href{http://www.numpy.org}{Numpy} --- a package for scientific
  computing.
\end{itemize}

If you have never worked with Tensorflow, you would probably need to
read some tutorials during your work on this assignment,
e.g.~\href{https://www.tensorflow.org/tutorials/recurrent}{this one}
could be a good starting point.

    \hypertarget{data}{%
\subsubsection{Data}\label{data}}

The following cell will download all data required for this assignment
into the folder \texttt{week2/data}.

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, max=849548), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, max=103771), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

    \end{Verbatim}

    
    \begin{verbatim}
HBox(children=(IntProgress(value=0, max=106837), HTML(value='')))
    \end{verbatim}

    
    \begin{Verbatim}[commandchars=\\\{\}]

    \end{Verbatim}

    \hypertarget{load-the-twitter-named-entity-recognition-corpus}{%
\subsubsection{Load the Twitter Named Entity Recognition
corpus}\label{load-the-twitter-named-entity-recognition-corpus}}

We will work with a corpus, which contains tweets with NE tags. Every
line of a file contains a pair of a token (word/punctuation symbol) and
a tag, separated by a whitespace. Different tweets are separated by an
empty line.

The function \emph{read\_data} reads a corpus from the \emph{file\_path}
and returns two lists: one with tokens and one with the corresponding
tags. You need to complete this function by adding a code, which will
replace a user's nickname to \texttt{\textless{}USR\textgreater{}} token
and any URL to \texttt{\textless{}URL\textgreater{}} token. You could
think that a URL and a nickname are just strings which start with
\emph{http://} or \emph{https://} in case of URLs and a *@* symbol for
nicknames.

    And now we can load three separate parts of the dataset: - \emph{train}
data for training the model; - \emph{validation} data for evaluation and
hyperparameters tuning; - \emph{test} data for final evaluation of the
model.

    You should always understand what kind of data you deal with. For this
purpose, you can print the data running the following cell:

    \begin{Verbatim}[commandchars=\\\{\}]
RT      O
<USR>   O
:       O
Online  O
ticket  O
sales   O
for     O
Ghostland       B-musicartist
Observatory     I-musicartist
extended        O
until   O
6       O
PM      O
EST     O
due     O
to      O
high    O
demand  O
.       O
Get     O
them    O
before  O
they    O
sell    O
out     O
{\ldots}     O

Apple   B-product
MacBook I-product
Pro     I-product
A1278   I-product
13.3    I-product
"       I-product
Laptop  I-product
-       I-product
MD101LL/A       I-product
(       O
June    O
,       O
2012    O
)       O
-       O
Full    O
read    O
by      O
eBay    B-company
<URL>   O
<URL>   O

Happy   O
Birthday        O
<USR>   O
!       O
May     O
Allah   B-person
s.w.t   O
bless   O
you     O
with    O
goodness        O
and     O
happiness       O
.       O

    \end{Verbatim}

    \hypertarget{prepare-dictionaries}{%
\subsubsection{Prepare dictionaries}\label{prepare-dictionaries}}

To train a neural network, we will use two mappings: -
\{token\}\(\to\)\{token id\}: address the row in embeddings matrix for
the current token; - \{tag\}\(\to\)\{tag id\}: one-hot ground truth
probability distribution vectors for computing the loss at the output of
the network.

Now you need to implement the function \emph{build\_dict} which will
return \{token or tag\}\(\to\)\{index\} and vice versa.

    After implementing the function \emph{build\_dict} you can make
dictionaries for tokens and tags. Special tokens in our case will be: -
\texttt{\textless{}UNK\textgreater{}} token for out of vocabulary
tokens; - \texttt{\textless{}PAD\textgreater{}} token for padding
sentence to the same length when we create batches of sentences.

    The next additional functions will help you to create the mapping
between tokens and ids for a sentence.

    \hypertarget{generate-batches}{%
\subsubsection{Generate batches}\label{generate-batches}}

Neural Networks are usually trained with batches. It means that weight
updates of the network are based on several sequences at every single
time. The tricky part is that all sequences within a batch need to have
the same length. So we will pad them with a special
\texttt{\textless{}PAD\textgreater{}} token. It is also a good practice
to provide RNN with sequence lengths, so it can skip computations for
padding parts. We provide the batching function
\emph{batches\_generator} readily available for you to save time.

    \hypertarget{build-a-recurrent-neural-network}{%
\subsection{Build a recurrent neural
network}\label{build-a-recurrent-neural-network}}

This is the most important part of the assignment. Here we will specify
the network architecture based on TensorFlow building blocks. It's fun
and easy as a lego constructor! We will create an LSTM network which
will produce probability distribution over tags for each token in a
sentence. To take into account both right and left contexts of the
token, we will use Bi-Directional LSTM (Bi-LSTM). Dense layer will be
used on top to perform tag classification.

    First, we need to create
\href{https://www.tensorflow.org/api_docs/python/tf/compat/v1/placeholder}{placeholders}
to specify what data we are going to feed into the network during the
execution time. For this task we will need the following placeholders: -
\emph{input\_batch} --- sequences of words (the shape equals to
{[}batch\_size, sequence\_len{]}); - \emph{ground\_truth\_tags} ---
sequences of tags (the shape equals to {[}batch\_size,
sequence\_len{]}); - \emph{lengths} --- lengths of not padded sequences
(the shape equals to {[}batch\_size{]}); - \emph{dropout\_ph} ---
dropout keep probability; this placeholder has a predefined value 1; -
\emph{learning\_rate\_ph} --- learning rate; we need this placeholder
because we want to change the value during training.

It could be noticed that we use \emph{None} in the shapes in the
declaration, which means that data of any size can be feeded.

You need to complete the function \emph{declare\_placeholders}.

    Now, let us specify the layers of the neural network. First, we need to
perform some preparatory steps:

\begin{itemize}
\tightlist
\item
  Create embeddings matrix with
  \href{https://www.tensorflow.org/api_docs/python/tf/Variable}{tf.Variable}.
  Specify its name (\emph{embeddings\_matrix}), type
  (\emph{tf.float32}), and initialize with random values.
\item
  Create forward and backward LSTM cells. TensorFlow provides a number
  of RNN cells ready for you. We suggest that you use \emph{LSTMCell},
  but you can also experiment with other types, e.g.~GRU cells.
  \href{http://colah.github.io/posts/2015-08-Understanding-LSTMs/}{This}
  blogpost could be interesting if you want to learn more about the
  differences.
\item
  Wrap your cells with
  \href{https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/DropoutWrapper}{DropoutWrapper}.
  Dropout is an important regularization technique for neural networks.
  Specify all keep probabilities using the dropout placeholder that we
  created before.
\end{itemize}

After that, you can build the computation graph that transforms an
input\_batch:

\begin{itemize}
\tightlist
\item
  \href{https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup}{Look
  up} embeddings for an \emph{input\_batch} in the prepared
  \emph{embedding\_matrix}.
\item
  Pass the embeddings through
  \href{https://www.tensorflow.org/api_docs/python/tf/nn/bidirectional_dynamic_rnn}{Bidirectional
  Dynamic RNN} with the specified forward and backward cells. Use the
  lengths placeholder here to avoid computations for padding tokens
  inside the RNN.
\item
  Create a dense layer on top. Its output will be used directly in loss
  function.
\end{itemize}

Fill in the code below. In case you need to debug something, the easiest
way is to check that tensor shapes of each step match the expected ones.

    To compute the actual predictions of the neural network, you need to
apply
\href{https://www.tensorflow.org/api_docs/python/tf/nn/softmax}{softmax}
to the last layer and find the most probable tags with
\href{https://www.tensorflow.org/api_docs/python/tf/argmax}{argmax}.

    During training we do not need predictions of the network, but we need a
loss function. We will use
\href{http://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html\#cross-entropy}{cross-entropy
loss}, efficiently implemented in TF as
\href{https://www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits_v2}{cross
entropy with logits}. Note that it should be applied to logits of the
model (not to softmax probabilities!). Also note, that we do not want to
take into account loss terms coming from
\texttt{\textless{}PAD\textgreater{}} tokens. So we need to mask them
out, before computing
\href{https://www.tensorflow.org/api_docs/python/tf/reduce_mean}{mean}.

    The last thing to specify is how we want to optimize the loss. We
suggest that you use
\href{https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer}{Adam}
optimizer with a learning rate from the corresponding placeholder. You
will also need to apply clipping to eliminate exploding gradients. It
can be easily done with
\href{https://www.tensorflow.org/api_docs/python/tf/clip_by_norm}{clip\_by\_norm}
function.

    Congratulations! You have specified all the parts of your network. You
may have noticed, that we didn't deal with any real data yet, so what
you have written is just recipes on how the network should function. Now
we will put them to the constructor of our Bi-LSTM class to use it in
the next section.

    \hypertarget{train-the-network-and-predict-tags}{%
\subsection{Train the network and predict
tags}\label{train-the-network-and-predict-tags}}

    \href{https://www.tensorflow.org/api_docs/python/tf/Session\#run}{Session.run}
is a point which initiates computations in the graph that we have
defined. To train the network, we need to compute \emph{self.train\_op},
which was declared in \emph{perform\_optimization}. To predict tags, we
just need to compute \emph{self.predictions}. Anyway, we need to feed
actual data through the placeholders that we defined before.

    Implement the function \emph{predict\_for\_batch} by initializing
\emph{feed\_dict} with input \emph{x\_batch} and \emph{lengths} and
running the \emph{session} for \emph{self.predictions}.

    We finished with necessary methods of our BiLSTMModel model and almost
ready to start experimenting.

\hypertarget{evaluation}{%
\subsubsection{Evaluation}\label{evaluation}}

To simplify the evaluation process we provide two functions for you: -
\emph{predict\_tags}: uses a model to get predictions and transforms
indices to tokens and tags; - \emph{eval\_conll}: calculates precision,
recall and F1 for the results.

    \hypertarget{run-your-experiment}{%
\subsection{Run your experiment}\label{run-your-experiment}}

    Create \emph{BiLSTMModel} model with the following parameters: -
\emph{vocabulary\_size} --- number of tokens; - \emph{n\_tags} ---
number of tags; - \emph{embedding\_dim} --- dimension of embeddings,
recommended value: 200; - \emph{n\_hidden\_rnn} --- size of hidden
layers for RNN, recommended value: 200; - \emph{PAD\_index} --- an index
of the padding token (\texttt{\textless{}PAD\textgreater{}}).

Set hyperparameters. You might want to start with the following
recommended values: - \emph{batch\_size}: 32; - 4 epochs; - starting
value of \emph{learning\_rate}: 0.005 - \emph{learning\_rate\_decay}: a
square root of 2; - \emph{dropout\_keep\_probability}: try several
values: 0.1, 0.5, 0.9.

However, feel free to conduct more experiments to tune hyperparameters
and earn extra points for the assignment.

    If you got an error \emph{``Tensor conversion requested dtype float64
for Tensor with dtype float32''} in this point, check if there are
variables without dtype initialised. Set the value of dtype equals to
\emph{tf.float32} for such variables.

    Finally, we are ready to run the training!

    Now let us see full quality reports for the final model on train,
validation, and test sets. To give you a hint whether you have
implemented everything correctly, you might expect F-score about 40\% on
the validation set.

\textbf{The output of the cell below (as well as the output of all the
other cells) should be present in the notebook for peer2peer review!}

    \hypertarget{conclusions}{%
\subsubsection{Conclusions}\label{conclusions}}

Could we say that our model is state of the art and the results are
acceptable for the task? Definately, we can say so. Nowadays, Bi-LSTM is
one of the state of the art approaches for solving NER problem and it
outperforms other classical methods. Despite the fact that we used small
training corpora (in comparison with usual sizes of corpora in Deep
Learning), our results are quite good. In addition, in this task there
are many possible named entities and for some of them we have only
several dozens of trainig examples, which is definately small. However,
the implemented model outperforms classical CRFs for this task. Even
better results could be obtained by some combinations of several types
of methods, e.g.~see \href{https://arxiv.org/abs/1603.01354}{this} paper
if you are interested.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
